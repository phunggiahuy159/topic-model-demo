import numpy as np
import pandas as pd
import sys
import nltk
import re
import gensim
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from gensim import corpora,models
from gensim.utils import simple_preprocess
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud 

data=pd.read_csv('D:\code\topic_modeling\abcnews-date-text.csv')
# take 5000 news as input data
data_samples=data['headline_text'][:5000]
data=data['headline_text'][:5000]

cleanedData = []
# print(data)
lemma = WordNetLemmatizer()
swords = stopwords.words("english")
for text in data:
    
    # Cleaning links
    text = re.sub(r'http\S+', '', text)
    
    # Cleaning everything except alphabetical and numerical characters
    text = re.sub("[^a-zA-Z0-9]"," ",text)
    
    # Tokenizing and lemmatizing
    text = nltk.word_tokenize(text.lower())
    text = [lemma.lemmatize(word) for word in text]
    
    # Removing stopwords
    text = [word for word in text if word not in swords]

    
    cleanedData.append(text)
# print(cleanedData)

dictionary = corpora.Dictionary(cleanedData)
# print(dictionary)
# print(cleanedData)
corpus = [dictionary.doc2bow(text) for text in cleanedData]
# print(corpus)
# print(dictionary[0])
# print(dictionary[1]
present_words=[]
docs = []
for doc in corpus:
    present_words=[]
    for j in doc:
        present_words.append(j[0])
    docs.append(present_words)

class LDA(object):
    
    def __init__(self,documents,vocabulary,K, max_iteration=1000):
        self.documents=documents
        # number of documents
        self.M=len(documents)
        self.vocabulary=vocabulary
        # number of unique words in documents
        self.V=len(self.vocabulary)
        print(self.V)
        # number of topics
        self.K = K
        # number of iterations in Gibbs sampling
        self.max_iteration = max_iteration
        # build the weight matrix for documents_topics, topic_words
        # Shape of documents_topics is M*K, Shape of topic_words is K*V
        # documents_sum_topics is the number of topics on each document
        # topics_sum_words is the number of words on each topic
        self.documents_topics=np.zeros((self.M,self.K))
        self.topics_words=np.zeros((self.K,self.V))
        self.documents_sum_topics=np.zeros((self.M))
        self.topics_sum_words=np.zeros((self.K))
        self.Z=[[0 for _ in range(len(d))] for d in self.documents] 
    
    def compute_prob(self,doc_index,word,topic,alpha=0.1,beta=0.1):
        # represent the probability of topic Z_k generated by the doc m, i.e. P(z|d,alpha)
        theta_m_k=(self.documents_topics[doc_index][topic]+alpha) / \
        (self.documents_sum_topics[doc_index]+self.K*alpha)
        # represent the probability of word v generated by the topic Z_k, i.e. P(w|z,beta)
        phi_k_v=(self.topics_words[topic][word]+beta) / \
        (self.topics_sum_words[topic]+self.V*beta)
        # compute P(z|d,alpha,beta)=P(z|d,alpha)*P(w|z,beta)
        prob=theta_m_k*phi_k_v
        
        return prob
        
    
    def gibbs_sampling(self):
        '''
        gibbs sampling
        '''
        topic_prob_list=[]
        for iterations in range(self.max_iteration):
            if iterations%100==0:
                print('iteration is:'+str(iterations))
            for i in range(self.M):
                for index,word in enumerate(self.documents[i]):
                    topic=self.Z[i][index]
                    # delete the topic from the weights
                    self.documents_topics[i][topic]-=1
                    self.documents_sum_topics[i]-=1
                    self.topics_words[topic][word]-=1
                    self.topics_sum_words[topic]-=1
                    # compute the probability of topics over the document
                    topic_probs=[self.compute_prob(i, word, k)
                            for k in range(self.K)]
                    
                    topic_probs=topic_probs/np.sum(topic_probs)
                    # choose a new topic
                    new_topic = np.random.multinomial(1, topic_probs).argmax()
                    # update the topic and add the weights
                    self.Z[i][index]=new_topic
                    self.documents_topics[i][new_topic]+=1
                    self.documents_sum_topics[i]+=1
                    self.topics_words[new_topic][word]+=1
                    self.topics_sum_words[new_topic]+=1
            theta_m = (self.documents_topics[i] + 0.1 )/ (self.documents_sum_topics[i] - 1 + self.K * 0.1)
            theta_m = theta_m/np.sum(theta_m)
            topic_prob_list.append(theta_m)
        return topic_prob_list
    
    
    def execute(self):
        # initialization for parameters
        for i in range(self.M):
            for index,word in enumerate(self.documents[i]):
                # choose a topic for the word based on the multinomial distribution
                self.Z[i][index]=np.random.multinomial(1,[1/self.K]*self.K).argmax()
                z=self.Z[i][index]
                # add the count to the weight matrix
                self.documents_topics[i][z]+=1
                self.documents_sum_topics[i]+=1
                self.topics_words[z][word]+=1
                self.topics_sum_words[z]+=1
        
        topic_prob_list=self.gibbs_sampling()
        return topic_prob_list
        

    def print_topics(self):
        '''
        print the 10 representative words and corresponding probability of all the topics
        '''
        topic_weights=self.topics_words/np.sum(self.topics_words,axis=1,keepdims=True)
        index_matrix=(topic_weights.argsort())[:,-10:]

        for i in range(index_matrix.shape[0]):
            strs=""
            print('topic '+'\n')
            for j in range(index_matrix.shape[1]):
                index=index_matrix[i][j]
                if j!=index_matrix.shape[1]-1:
                    strs=strs+np.around(topic_weights[i][index],3).astype('str')+'*'+self.vocabulary[index]+'+'
                else:
                    strs=strs+np.around(topic_weights[i][index],3).astype('str')+'*'+self.vocabulary[index]
            print(strs+'\n')
        
    def print_topic(self,k):
        '''
        print the 10 representative words and corresponding probability of a certain topic
        '''
        topic_weights=self.topics_words/np.sum(self.topics_words,axis=1,keepdims=True)
        index_matrix=(topic_weights.argsort())[:,-10:]
        strs=""
        print('topic '+str(k)+'\n')
        for j in range(index_matrix.shape[1]):
            index=index_matrix[k][j]
            if j!=index_matrix.shape[1]-1:
                strs=strs+np.around(topic_weights[k][index],3).astype('str')+'*'+self.vocabulary[index]+'+'
            else:
                strs=strs+np.around(topic_weights[k][index],3).astype('str')+'*'+self.vocabulary[index]
        print(strs+'\n')
        
    def plot_word_cloud(self,plt,flag=-1):
        '''
        plot the word cloud of the topics
        '''
        if flag==-1:
            for topic in range(self.K):
                data = []   
                text = ""
                topic_weights=self.topics_words/np.sum(self.topics_words,axis=1,keepdims=True)
                index_matrix=(topic_weights.argsort())[:,-10:]
                for i in range(len(self.topics_words[topic])):
                    if self.topics_words[topic][i]>1:
                        data.append(self.vocabulary[i]) 
                text = ' '.join(data)
                # Generate a word cloud image
                wordcloud = WordCloud().generate(text)  
                plt.figure()
                plt.imshow(wordcloud, interpolation='bilinear')
                plt.axis("off")
                plt.title("Topic #" + str(topic))
                plt.show()
        else:
            data = []   
            text = ""
            topic_weights=self.topics_words/np.sum(self.topics_words,axis=1,keepdims=True)
            index_matrix=(topic_weights.argsort())[:,-10:]
            for i in range(len(self.topics_words[flag])):
                if self.topics_words[flag][i]>1:
                    data.append(self.vocabulary[i]) 
            text = ' '.join(data)
            # Generate a word cloud image
            wordcloud = WordCloud().generate(text)  
            plt.figure()
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis("off")
            plt.title("Topic #" + str(topic))
            plt.show()
        
    def compute_perplexity(self):
        # compute topic-word probability
        topic_weights=self.topics_words/np.sum(self.topics_words,axis=1,keepdims=True)
        # compute doc-topic probability
        doc_topic_prob=self.documents_topics/np.sum(self.documents_topics,axis=1,keepdims=True)
        prob_doc=0
        doc_word_num=0
        for i in range(self.M):
            for index,word in enumerate(self.documents[i]):
                prob_word=0
                for k in range(self.K):
                    prob_topic=doc_topic_prob[i][k]
                    prob_topic_word=topic_weights[k][word]
                    prob_word +=prob_topic*prob_topic_word
                prob_doc+= np.log(prob_word)
            doc_word_num+=len(self.documents[i])
        
        perplexity=np.exp(-prob_doc/doc_word_num)
        print('the perplexity of LDA with '+str(self.K)+' topics is:'+str(perplexity))
        return perplexity
    
lda=LDA(documents=docs, vocabulary=dictionary, K=10, max_iteration=1000)
topic_prob_list=lda.execute()